{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Programming Assignment 4: Information Retrieval\n",
    "\n",
    "In this assignment you will be improving upon a rather poorly-made information retrieval system. You will build an inverted index to quickly retrieve documents that match queries and then make it even better by using term-frequency inverse-document-frequency weighting and cosine similarity to compare queries to your data set. Your IR system will be evaluated for accuracy on the correct documents retrieved for different queries and the correctly computed tf-idf values and cosine similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "You will be using your IR system to find relevant documents among a collection of sixty short stories by Rider Haggard. The training data is located in the data/ directory under the subdirectory RiderHaggard/. Within this directory you will see yet another directory raw/. This contains the raw text files of the sixty short stories. The data/ directory also contains the files dev_queries.txt and dev_solutions.txt. We have provided these to you as a set of development queries and their expected answers to use as you begin implementing your IR system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task\n",
    "\n",
    "Improve upon the given IR system by implementing the following features:\n",
    "\n",
    "<b>Inverted Positional Index:</b> Implement an inverted index - a mapping from words to the documents in which they occur, as well as the positions in the documents for which they occur.\n",
    "\n",
    "<b>Boolean Retrieval:</b> Implement a Boolean retrieval system, in which you return the list of documents that contain all words in a query. (Yes, you only need to support conjunctions for this assignment.)\n",
    "\n",
    "<b>Phrase Query Retrieval:</b> Implement a system that returns the list of documents in which the full phrase appears, (ie. the words of the query appear next to each other, in the specified order). Note that at the time of retrieval, you will not have access to the original documents anymore (the documents would be turned into bag of words), so you'll have to utilize your inverted positional index to complete this part.\n",
    "\n",
    "<b>TF-IDF:</b> Compute and store the term-frequency inverse-document-frequency value for every word-document co-occurrence:\n",
    "\n",
    "<b>Cosine Similarity:</b> Implement cosine similarity in order to improve upon the ranked retrieval system, which currently retrieves documents based upon the Jaccard coefficient between the query and each document. Also note that when computing $w_{t, q}$ (i.e. the weight for the word ùë§ in the query) do not include the idf term. That is, $w_{t, q} = 1 + \\log_{10} \\text{tf}_{t, q}$.\n",
    "<b> The reference solution uses ltc.lnn weighting for computing cosine scores. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Your IR system will be evaluated on a development set of queries as well as a held-out set of queries. The queries are encoded in the file <b>dev_queries.txt</b> and are:\n",
    "\n",
    "- separation of church and state\n",
    "- white-robed priests\n",
    "- ancient underground city\n",
    "- native african queen\n",
    "- zulu king\n",
    "\n",
    "We test your system based on the five parts mentioned above: the inverted index (used both to get word positions and to get postings), boolean retrieval, phrase query retrieval, computing the correct tf-idf values, and implementing cosine similarity using the tf-idf values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started!\n",
    "\n",
    "We will first start by importing some modules and setting up our IR system class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from porter_stemmer import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT CHANGE\n",
    "\"\"\"\n",
    "class IRSystem:\n",
    "    def __init__(self):\n",
    "        # For holding the data - initialized in read_data()\n",
    "        self.titles = []\n",
    "        self.docs = []\n",
    "        self.vocab = []\n",
    "        # For the text pre-processing.\n",
    "        self.alphanum = re.compile('[^a-zA-Z0-9]')\n",
    "        self.p = PorterStemmer()\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        uniq = set()\n",
    "        for doc in self.docs:\n",
    "            for word in doc:\n",
    "                uniq.add(word)\n",
    "        return uniq\n",
    "\n",
    "    def __read_raw_data(self, dirname):\n",
    "        print(\"Stemming Documents...\")\n",
    "\n",
    "        titles = []\n",
    "        docs = []\n",
    "        os.mkdir('%s/stemmed' % dirname)\n",
    "        title_pattern = re.compile('(.*) \\d+\\.txt')\n",
    "\n",
    "        # make sure we're only getting the files we actually want\n",
    "        filenames = []\n",
    "        for filename in os.listdir('%s/raw' % dirname):\n",
    "            if filename.endswith(\".txt\") and not filename.startswith(\".\"):\n",
    "                filenames.append(filename)\n",
    "\n",
    "        for i, filename in enumerate(filenames):\n",
    "            title = title_pattern.search(filename).group(1)\n",
    "            print(\"    Doc %d of %d: %s\" % (i + 1, len(filenames), title))\n",
    "            titles.append(title)\n",
    "            contents = []\n",
    "            f = open('%s/raw/%s' % (dirname, filename), 'r', encoding=\"utf-8\")\n",
    "            of = open('%s/stemmed/%s.txt' % (dirname, title), 'w',\n",
    "                      encoding=\"utf-8\")\n",
    "            for line in f:\n",
    "                # make sure everything is lower case\n",
    "                line = line.lower()\n",
    "                # split on whitespace\n",
    "                line = [xx.strip() for xx in line.split()]\n",
    "                # remove non alphanumeric characters\n",
    "                line = [self.alphanum.sub('', xx) for xx in line]\n",
    "                # remove any words that are now empty\n",
    "                line = [xx for xx in line if xx != '']\n",
    "                # stem words\n",
    "                line = [self.p.stem(xx) for xx in line]\n",
    "                # add to the document's conents\n",
    "                contents.extend(line)\n",
    "                if len(line) > 0:\n",
    "                    of.write(\" \".join(line))\n",
    "                    of.write('\\n')\n",
    "            f.close()\n",
    "            of.close()\n",
    "            docs.append(contents)\n",
    "        return titles, docs\n",
    "\n",
    "    def __read_stemmed_data(self, dirname):\n",
    "        print(\"Already stemmed!\")\n",
    "        titles = []\n",
    "        docs = []\n",
    "\n",
    "        # make sure we're only getting the files we actually want\n",
    "        filenames = []\n",
    "        for filename in os.listdir('%s/stemmed' % dirname):\n",
    "            if filename.endswith(\".txt\") and not filename.startswith(\".\"):\n",
    "                filenames.append(filename)\n",
    "\n",
    "        if len(filenames) != 60:\n",
    "            msg = \"There are not 60 documents in ../data/RiderHaggard/stemmed/\\n\"\n",
    "            msg += \"Remove ../data/RiderHaggard/stemmed/ directory and re-run.\"\n",
    "            raise Exception(msg)\n",
    "\n",
    "        for i, filename in enumerate(filenames):\n",
    "            title = filename.split('.')[0]\n",
    "            titles.append(title)\n",
    "            contents = []\n",
    "            f = open('%s/stemmed/%s' % (dirname, filename), 'r',\n",
    "                     encoding=\"utf-8\")\n",
    "            for line in f:\n",
    "                # split on whitespace\n",
    "                line = [xx.strip() for xx in line.split()]\n",
    "                # add to the document's conents\n",
    "                contents.extend(line)\n",
    "            f.close()\n",
    "            docs.append(contents)\n",
    "\n",
    "        return titles, docs\n",
    "\n",
    "    def read_data(self, dirname):\n",
    "        \"\"\"\n",
    "        Given the location of the 'data' directory, reads in the documents to\n",
    "        be indexed.\n",
    "        \"\"\"\n",
    "        # NOTE: We cache stemmed documents for speed\n",
    "        #       (i.e. write to files in new 'stemmed/' dir).\n",
    "\n",
    "        print(\"Reading in documents...\")\n",
    "        # dict mapping file names to list of \"words\" (tokens)\n",
    "        filenames = os.listdir(dirname)\n",
    "        subdirs = os.listdir(dirname)\n",
    "        if 'stemmed' in subdirs:\n",
    "            titles, docs = self.__read_stemmed_data(dirname)\n",
    "        else:\n",
    "            titles, docs = self.__read_raw_data(dirname)\n",
    "\n",
    "        # Sort document alphabetically by title to ensure we have the proper\n",
    "        # document indices when referring to them.\n",
    "        ordering = [idx for idx, title in sorted(enumerate(titles),\n",
    "                                                 key=lambda xx: xx[1])]\n",
    "\n",
    "        self.titles = []\n",
    "        self.docs = []\n",
    "        numdocs = len(docs)\n",
    "        for d in range(numdocs):\n",
    "            self.titles.append(titles[ordering[d]])\n",
    "            self.docs.append(docs[ordering[d]])\n",
    "\n",
    "        # Get the vocabulary.\n",
    "        self.vocab = [xx for xx in self.get_uniq_words()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will first build the inverted positional index (data structure that keeps track of the documents in which a particular word is contained, and the positions of that word in the document). The documents will have already been read in at this point. The following instance variables in the class are included in the starter code for you to use to build your inverted positional index: titles (a list of strings), docs (a list of lists of strings), and vocab (a list of strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def index(self):\n",
    "        \"\"\"\n",
    "        Build an index of the documents.\n",
    "        \"\"\"\n",
    "        print(\"Indexing...\")\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Create an inverted, positional index.\n",
    "        #       Granted this may not be a linked list as in a proper\n",
    "        #       implementation.\n",
    "        #       This index should allow easy access to both \n",
    "        #       1) the documents in which a particular word is contained, and \n",
    "        #       2) for every document, the positions of that word in the document \n",
    "        #       Some helpful instance variables:\n",
    "        #         * self.docs = List of documents\n",
    "        #         * self.titles = List of titles\n",
    "\n",
    "        inv_index = {}\n",
    "\n",
    "        # Generate inverted index here\n",
    "        for i in range(len(self.docs)):\n",
    "            curr_doc = self.docs[i] #List of words (the story)\n",
    "\n",
    "            for posting_idx in range(len(curr_doc)):\n",
    "                word = curr_doc[posting_idx]\n",
    "                if word not in inv_index:\n",
    "                    inv_index[word] = [0, {}]\n",
    "                doc_dict = inv_index[word][1]\n",
    "                if self.titles[i] not in doc_dict:\n",
    "                    doc_dict[self.titles[i]] = []\n",
    "                    inv_index[word][0] += 1\n",
    "                (doc_dict[self.titles[i]]).append(posting_idx)\n",
    "\n",
    "\n",
    "        self.inv_index = inv_index\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        # turn self.docs into a map from ID to bag of words\n",
    "        id_to_bag_of_words = {}\n",
    "        for d, doc in enumerate(self.docs):\n",
    "            bag_of_words = set(doc)\n",
    "            id_to_bag_of_words[d] = bag_of_words\n",
    "        self.docs = id_to_bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will implement get_word_positions. This method returns a list of integers that identifies the positions in the document doc in which the word is found.  This is basically just an API into your inverted index, but you must implement it in order for the index to be evaluated fully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def get_word_positions(self, word, doc):\n",
    "        \"\"\"\n",
    "        Given a word and a document, use the inverted index to return\n",
    "        the positions of the specified word in the specified document.\n",
    "        \"\"\"\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: return the list of positions for a word in a document.\n",
    "        positions = []\n",
    "        doc_title = self.titles[doc]\n",
    "        for position in self.inv_index[word][1][doc_title]:\n",
    "            positions.append(position)\n",
    "\n",
    "        return positions\n",
    "        # ------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add another method, get_posting, that returns a list of integers (document IDs) that identifies the documents in which the word is found. This is basically another API into your inverted index, but you must implement it in order to be evaluated fully.\n",
    "\n",
    "Keep in mind that the document IDs in each postings list to be sorted in order to perform the linear merge for boolean retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement Boolean retrieval, returning a list of document IDs corresponding to the documents in which all the words in query occur.\n",
    "\n",
    "Please implement the linear merge algorithm outlined in the videos/book (do not use built-in set intersection functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def get_posting(self, word):\n",
    "        \"\"\"\n",
    "        Given a word, this returns the list of document indices (sorted) in\n",
    "        which the word occurs.\n",
    "        \"\"\"\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: return the list of postings for a word.\n",
    "        posting = []\n",
    "        doc_titles= []\n",
    "        for title in self.inv_index[word][1]:\n",
    "            doc_titles.append(title)\n",
    "\n",
    "        for idx in range(len(self.titles)):\n",
    "            if self.titles[idx] in doc_titles:\n",
    "                posting.append(idx)\n",
    "\n",
    "        return posting\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "    def get_posting_unstemmed(self, word):\n",
    "        \"\"\"\n",
    "        Given a word, this *stems* the word and then calls get_posting on the\n",
    "        stemmed word to get its postings list. You should *not* need to change\n",
    "        this function. It is needed for submission.\n",
    "        \"\"\"\n",
    "        word = self.p.stem(word)\n",
    "        return self.get_posting(word)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def boolean_retrieve(self, query):\n",
    "        \"\"\"\n",
    "        Given a query in the form of a list of *stemmed* words, this returns\n",
    "        the list of documents in which *all* of those words occur (ie an AND\n",
    "        query).\n",
    "        Return an empty list if the query does not return any documents.\n",
    "        \"\"\"\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Implement Boolean retrieval. You will want to use your\n",
    "        #       inverted index that you created in index().\n",
    "        # Right now this just returns all the possible documents!\n",
    "        min_num_docs = 61 # There are a total of 60 documents\n",
    "        min_doc_word = ''\n",
    "        for word in query:\n",
    "            if self.inv_index[word][0] < min_num_docs:\n",
    "                min_doc_word = word\n",
    "\n",
    "        def isOccuring(title):\n",
    "            for word in query:\n",
    "                if title not in self.inv_index[word][1]:\n",
    "                    return False\n",
    "            return True\n",
    "\n",
    "        docs_titles = []\n",
    "        for title in self.inv_index[min_doc_word][1]:\n",
    "            if isOccuring(title):\n",
    "                docs_titles.append(title)\n",
    "\n",
    "        docs = [] # List of docIDs\n",
    "        for idx in range(len(self.titles)):\n",
    "            if self.titles[idx] in docs_titles:\n",
    "                docs.append(idx)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        return sorted(docs)  # sorted doesn't actually matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue to phase query retrieval. Our phrase_retrieve method will return a list of document IDs corresponding to the documents in which all the actual query phrase occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def phrase_retrieve(self, query):\n",
    "        \"\"\"\n",
    "        Given a query in the form of an ordered list of *stemmed* words, this \n",
    "        returns the list of documents in which *all* of those words occur, and \n",
    "        in the specified order. \n",
    "        Return an empty list if the query does not return any documents. \n",
    "        \"\"\"\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Implement Phrase Query retrieval (ie. return the documents \n",
    "        #       that don't just contain the words, but contain them in the \n",
    "        #       correct order) You will want to use the inverted index \n",
    "        #       that you created in index(), and may also consider using\n",
    "        #       boolean_retrieve. \n",
    "        #       NOTE that you no longer have access to the original documents\n",
    "        #       in self.docs because it is now a map from doc IDs to set\n",
    "        #       of unique words in the original document.\n",
    "        # Right now this just returns all possible documents!\n",
    "\n",
    "        potential_docIDs = self.boolean_retrieve(query)\n",
    "        potential_titles = []\n",
    "        for docID in potential_docIDs:\n",
    "            potential_titles.append(self.titles[docID])\n",
    "\n",
    "        def isPhrase(query, title, doc_titles):\n",
    "            all_postings = []\n",
    "            for word in query:\n",
    "                postings_list = self.inv_index[word][1][title].copy()\n",
    "                all_postings.append(postings_list)\n",
    "            for posting in all_postings[0]:\n",
    "                recursive_find(all_postings, posting, 1, doc_titles, title)\n",
    "\n",
    "\n",
    "        def recursive_find(all_postings, posting, depth, doc_titles, title):\n",
    "            if depth == len(all_postings):\n",
    "                doc_titles.append(title)\n",
    "            else:\n",
    "                if posting + 1 in all_postings[depth]:\n",
    "                    recursive_find(all_postings, posting + 1, depth + 1, doc_titles, title)\n",
    "\n",
    "        doc_titles = []\n",
    "        for title in potential_titles:\n",
    "            isPhrase(query, title, doc_titles)\n",
    "\n",
    "\n",
    "        docs = [] # List of docIDs\n",
    "        for idx in range(len(self.titles)):\n",
    "            if self.titles[idx] in doc_titles:\n",
    "                docs.append(idx)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        return sorted(docs)  # sorted doesn't actually matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute and score the td-idf values. compute_tfidf and stores the tf-idf values for words and documents. For this you will probably want to be aware of the class variable vocab, which holds the list of all unique words, as well as the inverted index you created earlier.\n",
    "\n",
    "You must also implement get_tfidf to return the tf-idf weight for a particular word and document ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def compute_tfidf(self):\n",
    "        # -------------------------------------------------------------------\n",
    "        # TODO: Compute and store TF-IDF values for words and documents in self.tfidf.\n",
    "        #       Recall that you can make use of:\n",
    "        #         * self.vocab: a list of all distinct (stemmed) words\n",
    "        #         * self.docs: a list of lists, where the i-th document is\n",
    "        #                   self.docs[i] => ['word1', 'word2', ..., 'wordN']\n",
    "        #       NOTE that you probably do *not* want to store a value for every\n",
    "        #       word-document pair, but rather just for those pairs where a\n",
    "        #       word actually occurs in the document.\n",
    "        print(\"Calculating tf-idf...\")\n",
    "        self.tfidf = {}\n",
    "        N = len(self.docs)\n",
    "\n",
    "        for word in self.vocab:\n",
    "            for title in self.inv_index[word][1]:\n",
    "                tf = 1 + math.log(len(self.inv_index[word][1][title]), 10)\n",
    "                idf = math.log(N / (self.inv_index[word][0]), 10)\n",
    "                self.tfidf[(word, title)] = tf * idf\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "    def get_tfidf(self, word, document):\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Return the tf-idf weigthing for the given word (string) and\n",
    "        #       document index.\n",
    "        tfidf = self.tfidf[(word, self.titles[document])]\n",
    "        # ------------------------------------------------------------------\n",
    "        return tfidf\n",
    "\n",
    "    def get_tfidf_unstemmed(self, word, document):\n",
    "        \"\"\"\n",
    "        This function gets the TF-IDF of an *unstemmed* word in a document.\n",
    "        Stems the word and then calls get_tfidf. You should *not* need to\n",
    "        change this interface, but it is necessary for submission.\n",
    "        \"\"\"\n",
    "        word = self.p.stem(word)\n",
    "        return self.get_tfidf(word, document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will implement rank_retrieve. This function returns a priority queue of the top ranked documents for a given query. Right now it ranks documents according to their Jaccard similarity with the query, but you will replace this method of ranking with a ranking using the <b>cosine similarity</b> between the documents and query.\n",
    "    \n",
    "Remember to use ltc.lnn weighting, that is, ltc weighting for the document and lnn weighting for the query! This means that the query vector weights will be $1 + \\log_{10} \\text{tf}_{t, q}$ with no IDF term or normalization, but we normalize the document vector weights by the length of the document vector (square root of the sum of squares of the tf-idf weights). Finally, the cosine scores are the dot product of the query vector and the document vectors. Refer to this [handout](http://web.stanford.edu/class/cs124/lec/CS124_IR_Handout.pdf) or lecture slides for a more detailed explanation.\n",
    "    \n",
    "When we say normalize by \"document length\" or \"length of document\", we mean the length of the document vector, NOT the number of words in the actual text document. So, when you‚Äôre computing cosine similarity between the query and document, the document vector is the tf.idf document weights for all terms in the vocabulary. The document length would be the L2 norm of the document vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def rank_retrieve(self, query):\n",
    "        \"\"\"\n",
    "        Given a query (a list of words), return a rank-ordered list of\n",
    "        documents (by ID) and score for the query.\n",
    "        \"\"\"\n",
    "        scores = [0.0 for xx in range(len(self.titles))]\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Implement cosine similarity between a document and a list of\n",
    "        #       query words.\n",
    "\n",
    "        # Right now, this code simply gets the score by taking the Jaccard\n",
    "        # similarity between the query and every document.\n",
    "        words_in_query = set()\n",
    "        for word in query:\n",
    "            words_in_query.add(word)\n",
    "\n",
    "        query_word_dict = {}\n",
    "        for word in query:\n",
    "            if word not in query_word_dict:\n",
    "                query_word_dict[word] = 0\n",
    "            query_word_dict[word] += 1\n",
    "\n",
    "        ltc_map = {}\n",
    "        for word in words_in_query:\n",
    "            ltc_map[word] = 1 + math.log(query_word_dict[word], 10)\n",
    "\n",
    "\n",
    "        for d in range(len(self.docs)):\n",
    "            doc_title = self.titles[d]\n",
    "            sum_weight = 0\n",
    "            words_in_doc = self.docs[d]\n",
    "            for word in words_in_doc:\n",
    "                sum_weight += (self.tfidf[(word, doc_title)])**2\n",
    "            length_doc = math.sqrt(sum_weight)\n",
    "\n",
    "            score = 0\n",
    "            for word in words_in_query.intersection(words_in_doc):\n",
    "                score += ltc_map[word] * (self.tfidf[(word, doc_title)] / length_doc)\n",
    "\n",
    "            scores[d] = score\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        ranking = [idx for idx, sim in sorted(enumerate(scores),\n",
    "                                              key=lambda xx: xx[1],\n",
    "                                              reverse=True)]\n",
    "        results = []\n",
    "        for i in range(10):\n",
    "            results.append((ranking[i], scores[ranking[i]]))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also add a few methods that given a string, will process and then return the list of matching documents for the different methods you have implemented. You do not need to add any additional code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT CHANGE\n",
    "\"\"\"\n",
    "class IRSystem(IRSystem):\n",
    "    def process_query(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a query string, process it and return the list of lowercase,\n",
    "        alphanumeric, stemmed words in the string.\n",
    "        \"\"\"\n",
    "        # make sure everything is lower case\n",
    "        query = query_str.lower()\n",
    "        # split on whitespace\n",
    "        query = query.split()\n",
    "        # remove non alphanumeric characters\n",
    "        query = [self.alphanum.sub('', xx) for xx in query]\n",
    "        # stem words\n",
    "        query = [self.p.stem(xx) for xx in query]\n",
    "        return query\n",
    "\n",
    "    def query_retrieve(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a string, process and then return the list of matching documents\n",
    "        found by boolean_retrieve().\n",
    "        \"\"\"\n",
    "        query = self.process_query(query_str)\n",
    "        return self.boolean_retrieve(query)\n",
    "\n",
    "    def phrase_query_retrieve(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a string, process and then return the list of matching documents\n",
    "        found by phrase_retrieve().\n",
    "        \"\"\"\n",
    "        query = self.process_query(query_str)\n",
    "        return self.phrase_retrieve(query)\n",
    "\n",
    "    def query_rank(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a string, process and then return the list of the top matching\n",
    "        documents, rank-ordered.\n",
    "        \"\"\"\n",
    "        query = self.process_query(query_str)\n",
    "        return self.rank_retrieve(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the code\n",
    "You can use the run_query function to test your code on a specific query. \n",
    "\n",
    "Note that the first time to run the run_query function, it will create a directory named stemmed/ in ../data/RiderHaggard/. This is meant to be a simple cache for the raw text documents. Later runs will be much faster after the first run. However, this means that if something happens during this first run and it does not get through processing all the documents, you may be left with an incomplete set of documents in ../data/RiderHaggard/stemmed/. If this happens, simply remove the stemmed/ directory and re-run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT CHANGE\n",
    "\"\"\"\n",
    "def run_tests(irsys):\n",
    "    print(\"===== Running tests =====\")\n",
    "\n",
    "    ff = open('./data/dev_queries.txt')\n",
    "    questions = [xx.strip() for xx in ff.readlines()]\n",
    "    ff.close()\n",
    "    ff = open('./data/dev_solutions.txt')\n",
    "    solutions = [xx.strip() for xx in ff.readlines()]\n",
    "    ff.close()\n",
    "\n",
    "    total_score = 0\n",
    "\n",
    "    epsilon = 1e-4\n",
    "    for part in range(6):\n",
    "        points = 0\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "\n",
    "        prob = questions[part]\n",
    "        soln = json.loads(solutions[part])\n",
    "\n",
    "        if part == 0:  # inverted index test\n",
    "            print(\"Inverted Index Test\")\n",
    "            queries = prob.split(\"; \")\n",
    "            queries = [xx.split(\", \") for xx in queries]\n",
    "            queries = [(xx[0], int(xx[1])) for xx in queries]\n",
    "            for i, (word, doc) in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.get_word_positions(word, doc)\n",
    "                if sorted(guess) == soln[i]:\n",
    "                    num_correct += 1\n",
    "\n",
    "        if part == 1:  # get postings test\n",
    "            print(\"Get Postings Test\")\n",
    "            words = prob.split(\", \")\n",
    "            for i, word in enumerate(words):\n",
    "                num_total += 1\n",
    "                posting = irsys.get_posting_unstemmed(word)\n",
    "                if posting == soln[i]:\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 2:  # boolean retrieval test\n",
    "            print(\"Boolean Retrieval Test\")\n",
    "            queries = prob.split(\", \")\n",
    "            for i, query in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.query_retrieve(query)\n",
    "                if set(guess) == set(soln[i]):\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 3:  # phrase query test\n",
    "            print(\"Phrase Query Retrieval\")\n",
    "            queries = prob.split(\", \")\n",
    "            for i, query in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.phrase_query_retrieve(query)\n",
    "                if set(guess) == set(soln[i]):\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 4:  # tfidf test\n",
    "            print(\"TF-IDF Test\")\n",
    "            queries = prob.split(\"; \")\n",
    "            queries = [xx.split(\", \") for xx in queries]\n",
    "            queries = [(xx[0], int(xx[1])) for xx in queries]\n",
    "            for i, (word, doc) in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.get_tfidf_unstemmed(word, doc)\n",
    "                if guess >= float(soln[i]) - epsilon and guess <= float(soln[i]) + epsilon:\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 5:  # cosine similarity test\n",
    "            print(\"Cosine Similarity Test\")\n",
    "            queries = prob.split(\", \")\n",
    "            for i, query in enumerate(queries):\n",
    "                num_total += 1\n",
    "                ranked = irsys.query_rank(query)\n",
    "                top_rank = ranked[0]\n",
    "                if top_rank[0] == soln[i][0]:\n",
    "                    if top_rank[1] >= float(soln[i][1]) - epsilon and top_rank[1] <= float(soln[i][1]) + epsilon:\n",
    "                        num_correct += 1\n",
    "\n",
    "        feedback = \"%d/%d Correct. Accuracy: %f\" % (num_correct, num_total, float(num_correct) / num_total)\n",
    "\n",
    "        if part == 1:\n",
    "            if num_correct == num_total:\n",
    "                points = 2\n",
    "            elif num_correct >= 0.5 * num_total:\n",
    "                points = 1\n",
    "            else:\n",
    "                points = 0\n",
    "        elif part == 2:\n",
    "            if num_correct == num_total:\n",
    "                points = 1\n",
    "            else:\n",
    "                points = 0\n",
    "        else:\n",
    "            if num_correct == num_total:\n",
    "                points = 3\n",
    "            elif num_correct > 0.75 * num_total:\n",
    "                points = 2\n",
    "            elif num_correct > 0:\n",
    "                points = 1\n",
    "            else:\n",
    "                points = 0\n",
    "        total_score += points\n",
    "        print(\"    Score: %d Feedback: %s\" % (points, feedback))\n",
    "\n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in documents...\n",
      "Already stemmed!\n",
      "Indexing...\n",
      "Calculating tf-idf...\n",
      "===== Running tests =====\n",
      "Inverted Index Test\n",
      "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
      "Get Postings Test\n",
      "    Score: 2 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
      "Boolean Retrieval Test\n",
      "    Score: 1 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
      "Phrase Query Retrieval\n",
      "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
      "TF-IDF Test\n",
      "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
      "Cosine Similarity Test\n",
      "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
      "\n",
      "Final Score: 100 out of 100\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DO NOT CHANGE\n",
    "\"\"\"\n",
    "## Run this cell to run the tests\n",
    "irsys = IRSystem()\n",
    "irsys.read_data('./data/RiderHaggard')\n",
    "irsys.index()\n",
    "irsys.compute_tfidf()\n",
    "\n",
    "import math\n",
    "\n",
    "total_score = 15\n",
    "final_score = math.ceil(100 * run_tests(irsys) / total_score)\n",
    "print(f\"\\nFinal Score: {final_score} out of 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in documents...\n",
      "Already stemmed!\n",
      "Indexing...\n",
      "Calculating tf-idf...\n",
      "Best matching documents to 'My very own query':\n",
      "Elissa: 1.321682e-02\n",
      "Heu-Heu (1924): 1.152362e-02\n",
      "The Ghost Kings: 1.137714e-02\n",
      "Stories by English Authors Africa (Selected by Scribners): 1.124628e-02\n",
      "Love Eternal: 1.076495e-02\n",
      "Lysbeth, a Tale of the Dutch: 9.444311e-03\n",
      "The Lady of Blossholme: 9.299776e-03\n",
      "Pearl-Maiden: 9.241841e-03\n",
      "Stella Fregelius: 8.908197e-03\n",
      "Moon of Israel: 8.860210e-03\n"
     ]
    }
   ],
   "source": [
    "def run_query(query):\n",
    "    irsys = IRSystem()\n",
    "    irsys.read_data('./data/RiderHaggard')\n",
    "    irsys.index()\n",
    "    irsys.compute_tfidf()\n",
    "\n",
    "    print(\"Best matching documents to '%s':\" % query)\n",
    "    results = irsys.query_rank(query)\n",
    "    for docId, score in results:\n",
    "        print(\"%s: %e\" % (irsys.titles[docId], score))\n",
    "\n",
    "\n",
    "## Example query run where \"My very own query\" is your query.\n",
    "## TODO test your own query\n",
    "run_query(\"My very own query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval Systems and Search Engines\n",
    "\n",
    "Safiya Umoja Noble‚Äôs <a href=\"https://nyupress.org/9781479837243/algorithms-of-oppression/\">Algorithms of Oppression</a> (2018) provides insight into how search engines and information retrieval algorithms can exhibit substantial racist and sexist biases. Noble demonstrates how prejudice against black women is embedded into search engine ranked results. These biases are apparent in both Google search‚Äôs autosuggestions and the first page of Google results. In this assignment, we have explored numerous features that are built into information retrieval systems, like Google Search. \n",
    "\n",
    "How could the algorithms you built in this assignment contribute to enforcing real-world biases? \n",
    "\n",
    "How can we reduce data discrimination and algorithmic bias that perpetuate gender and racial inequalities in search results and IR system?\n",
    "\n",
    "Please provide a short response to each of these questions (1-2 paragraphs per question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def algorithmic_bias_in_IR_systems(self):\n",
    "        # TODO: Place your response into the response string below\n",
    "        response = \"Q1: Our algorithm's training data is based off of the work of H.Rider Haggard. His novels were written in the 19th and early 20th century, and revolved around adventures in 'exotic locations'. His works have been criticized for their depiciting non-Europeans in a racist and misogynistic manner. In turn, our algorithm learns this behavior and is more likely to correlate racial minorities and women with negative traits. Q2: We can reduce data descrimination and algorithmic bias by carefully identifying the source of bias in the data set and removing it. We should also use a wider set of data that covers multiple persepctives and the opinions of marginalized communities.\"\n",
    "        return response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
